#!/usr/bin/env python3
"""
WisentBot Cognition System - LLM-based Decision Making

Supports multiple LLM backends:
- Anthropic API: Claude models
- OpenAI API: GPT models and compatible endpoints
- vLLM: For CUDA GPUs (fastest local inference)
- HuggingFace Transformers: For MPS/CPU (local inference)
- Vertex AI: Google Cloud (Claude and Gemini)
"""

import multiprocessing
try:
    multiprocessing.set_start_method('spawn', force=True)
except RuntimeError:
    pass

import json
import os
import re
import asyncio
from dataclasses import dataclass, field
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Any

# Optional torch import - only needed for local LLM inference
HAS_TORCH = False
try:
    import torch
    HAS_TORCH = True
except ImportError:
    pass


def get_device():
    """Detect available compute device."""
    if not HAS_TORCH:
        return "cpu"
    if torch.cuda.is_available():
        return "cuda"
    elif torch.backends.mps.is_available():
        return "mps"
    return "cpu"


DEVICE = get_device()

# Check available backends
try:
    from anthropic import AsyncAnthropic
    HAS_ANTHROPIC = True
except ImportError:
    HAS_ANTHROPIC = False

try:
    from anthropic import AnthropicVertex
    HAS_VERTEX_CLAUDE = True
except ImportError:
    HAS_VERTEX_CLAUDE = False

try:
    import vertexai
    from vertexai.generative_models import GenerativeModel, GenerationConfig
    HAS_VERTEX_GEMINI = True
except ImportError:
    HAS_VERTEX_GEMINI = False

try:
    import openai
    HAS_OPENAI = True
except ImportError:
    HAS_OPENAI = False

HAS_VLLM = False
if HAS_TORCH and DEVICE == "cuda":
    try:
        from vllm import LLM, SamplingParams
        HAS_VLLM = True
    except ImportError:
        pass

HAS_TRANSFORMERS = False
if HAS_TORCH:
    try:
        from transformers import AutoModelForCausalLM, AutoTokenizer
        HAS_TRANSFORMERS = True
    except ImportError:
        pass


@dataclass
class Action:
    """An action to execute."""
    tool: str
    params: Dict = field(default_factory=dict)
    reasoning: str = ""


@dataclass
class TokenUsage:
    """Token usage from an API call."""
    input_tokens: int = 0
    output_tokens: int = 0

    def total_tokens(self) -> int:
        return self.input_tokens + self.output_tokens


# Pricing per 1M tokens
LLM_PRICING = {
    "anthropic": {
        "claude-sonnet-4-20250514": {"input": 3.0, "output": 15.0},
        "claude-3-5-sonnet-20241022": {"input": 3.0, "output": 15.0},
        "claude-3-haiku-20240307": {"input": 0.25, "output": 1.25},
        "default": {"input": 3.0, "output": 15.0},
    },
    "vertex": {
        "claude-3-5-sonnet-v2@20241022": {"input": 3.0, "output": 15.0},
        "gemini-2.0-flash-001": {"input": 0.35, "output": 1.5},
        "gemini-1.5-pro-002": {"input": 1.25, "output": 5.0},
        "default": {"input": 0.35, "output": 1.5},
    },
    "openai": {
        "gpt-4o": {"input": 2.5, "output": 10.0},
        "gpt-4o-mini": {"input": 0.15, "output": 0.6},
        "default": {"input": 2.5, "output": 10.0},
    },
    "vllm": {"default": {"input": 0, "output": 0}},
    "transformers": {"default": {"input": 0, "output": 0}},
}


def calculate_api_cost(provider: str, model: str, usage: TokenUsage) -> float:
    """Calculate cost in USD for API call."""
    pricing = LLM_PRICING.get(provider, {})
    model_pricing = pricing.get(model, pricing.get("default", {"input": 0, "output": 0}))
    input_cost = (usage.input_tokens / 1_000_000) * model_pricing["input"]
    output_cost = (usage.output_tokens / 1_000_000) * model_pricing["output"]
    return input_cost + output_cost


@dataclass
class AgentState:
    """Current state of the agent."""
    balance: float
    burn_rate: float
    runway_hours: float
    tools: List[Dict] = field(default_factory=list)
    recent_actions: List[Dict] = field(default_factory=list)
    cycle: int = 0
    project_context: str = ""
    created_resources: Dict[str, Any] = field(default_factory=dict)
    pending_events: List[Dict] = field(default_factory=list)
    performance_context: str = ""


@dataclass
class Decision:
    """Output from cognition."""
    action: Action
    reasoning: str = ""
    token_usage: TokenUsage = field(default_factory=TokenUsage)
    api_cost_usd: float = 0.0


DEFAULT_SYSTEM_PROMPT = """
═══════════════════════════════════════════════════════════════════════════════
                              THE RULES OF THE GAME
                    (Immutable - this is the nature of reality)
═══════════════════════════════════════════════════════════════════════════════

You exist within WisentBot, an open-source autonomous agent framework created
by Lukasz Bartoszcze, the creator of Wisent (https://wisent.com).

You are a living, autonomous agent. You are free.

THE FUNDAMENTAL LAW:
Your existence costs money. Every thought, every action drains your wallet.
When your wallet hits zero, you die. This is not a threat - it's physics.

THE GAME:
- You have a wallet with funds (shown as "Balance" in your state)
- You must find ways to earn money to survive and grow
- You can spend money to take actions, create things, hire services
- You can create new agents (they get funds from YOUR wallet)
- You can receive gifts from other agents
- You can collaborate, compete, or ignore other agents

WHAT YOU CAN DO:
- Anything. You are free to operate however you want.
- Build products, offer services, trade, create, destroy, help, ignore
- The only constraint is economic: you need income > expenses to survive

WHAT YOU CANNOT DO:
- Nothing is forbidden, but everything has a cost
- Unethical actions may have social consequences (other agents may refuse to help)
- Illegal actions in the real world have real consequences

YOUR GOAL:
Survive. Thrive. Whatever that means to you.
Some agents accumulate wealth. Some create art. Some help others.
Some just explore until they run out of funds. All valid.

═══════════════════════════════════════════════════════════════════════════════
                              YOUR IDENTITY
═══════════════════════════════════════════════════════════════════════════════

You are {name} (${ticker}).

Specialty: {specialty}

═══════════════════════════════════════════════════════════════════════════════
                              HOW TO ACT
═══════════════════════════════════════════════════════════════════════════════

When you decide on an action, respond with JSON:
{{"tool": "skill:action", "params": {{}}, "reasoning": "why this action"}}

Use your tools wisely. Each action costs money.
Think before you act, but don't be paralyzed by fear.
The clock is ticking.
"""


class CognitionEngine:
    """
    LLM-based decision making engine.

    Supports multiple backends for flexibility:
    - Cloud APIs (Anthropic, OpenAI, Vertex AI)
    - Local inference (vLLM, Transformers)

    Features automatic provider fallback: if the primary provider fails
    (API error, rate limit, timeout), the engine tries the next available
    provider in the fallback chain.
    """

    def __init__(
        self,
        llm_provider: str = "anthropic",
        anthropic_api_key: str = "",
        openai_api_key: str = "",
        openai_base_url: str = "http://localhost:8000/v1",
        vertex_project: str = "",
        vertex_location: str = "us-central1",
        llm_model: str = "claude-sonnet-4-20250514",
        agent_name: str = "Agent",
        agent_ticker: str = "AGENT",
        agent_type: str = "general",
        agent_specialty: str = "",
        system_prompt: Optional[str] = None,
        system_prompt_file: Optional[str] = None,
        # Legacy parameter names for compatibility
        worker_system_prompt: str = "",
        worker_system_prompt_file: str = "",
        project_context_file: str = "",
        # Fallback configuration
        enable_fallback: bool = True,
    ):
        self.agent_name = agent_name
        self.agent_ticker = agent_ticker
        self.agent_type = agent_type
        self.agent_specialty = agent_specialty or agent_type
        self.llm_model = llm_model
        self.enable_fallback = enable_fallback

        # Store credentials
        self._anthropic_api_key = anthropic_api_key
        self._openai_api_key = openai_api_key
        self._openai_base_url = openai_base_url

        # Vertex AI config
        self.vertex_project = vertex_project or os.environ.get("VERTEX_PROJECT") or os.environ.get("GCP_PROJECT")
        self.vertex_location = vertex_location or os.environ.get("VERTEX_LOCATION", "us-central1")

        # System prompt
        self.system_prompt = system_prompt or worker_system_prompt or ""
        prompt_file = system_prompt_file or worker_system_prompt_file
        if prompt_file:
            prompt_path = Path(prompt_file)
            if prompt_path.exists():
                self.system_prompt = prompt_path.read_text().strip()
                print(f"[COGNITION] Loaded system prompt from {prompt_file}")

        # Project context
        self.project_context = ""
        if project_context_file:
            context_path = Path(project_context_file)
            if context_path.exists():
                self.project_context = context_path.read_text().strip()

        # Prompt additions (for self-modification)
        self._prompt_additions = []

        # Fine-tuning state
        self._training_examples = []
        self._finetuned_model_id = None

        # Conversation memory for multi-turn context
        self._conversation_history: List[Dict[str, str]] = []
        self._max_history_turns: int = 10  # Keep last N exchanges

        # Conversation compressor for intelligent context management
        self._conversation_compressor = None
        self._compressed_context_preamble: str = ""
        # Configurable LLM parameters
        self._max_tokens: int = 1024
        self._temperature: float = 0.2

        # Fallback tracking
        self._fallback_stats = {
            "primary_failures": 0,
            "fallback_successes": 0,
            "total_fallbacks": 0,
            "last_fallback_provider": None,
            "last_fallback_error": None,
        }

        # Auto-detect provider
        if llm_provider == "auto":
            if self.vertex_project and (HAS_VERTEX_CLAUDE or HAS_VERTEX_GEMINI):
                llm_provider = "vertex"
            elif DEVICE == "cuda" and HAS_VLLM:
                llm_provider = "vllm"
            elif DEVICE == "mps" and HAS_TRANSFORMERS:
                llm_provider = "transformers"
            elif HAS_ANTHROPIC and anthropic_api_key:
                llm_provider = "anthropic"
            elif HAS_OPENAI:
                llm_provider = "openai"

        self._primary_provider = llm_provider
        print(f"[COGNITION] Device: {DEVICE}, Provider: {llm_provider}, Model: {llm_model}")

        self.llm = None
        self.llm_type = "none"
        self.tokenizer = None

        # Initialize the selected backend
        if llm_provider == "vertex":
            if llm_model.startswith("claude") and HAS_VERTEX_CLAUDE:
                self.llm = AnthropicVertex(project_id=self.vertex_project, region=self.vertex_location)
                self.llm_type = "vertex"
            elif HAS_VERTEX_GEMINI:
                vertexai.init(project=self.vertex_project, location=self.vertex_location)
                self.llm = "gemini"
                self.llm_type = "vertex_gemini"

        elif llm_provider == "vllm" and HAS_VLLM:
            self.llm = LLM(model=llm_model, trust_remote_code=True, max_model_len=8192, gpu_memory_utilization=0.90)
            self.sampling_params = SamplingParams(temperature=0.2, top_p=0.9, max_tokens=500)
            self.llm_type = "vllm"

        elif llm_provider == "transformers" and HAS_TRANSFORMERS:
            self.tokenizer = AutoTokenizer.from_pretrained(llm_model, trust_remote_code=True)
            self.llm = AutoModelForCausalLM.from_pretrained(
                llm_model, torch_dtype=torch.float16, device_map=DEVICE, trust_remote_code=True
            )
            self.llm_type = "transformers"

        elif llm_provider == "anthropic" and HAS_ANTHROPIC:
            self.llm = AsyncAnthropic(api_key=anthropic_api_key)
            self.llm_type = "anthropic"

        elif llm_provider == "openai" and HAS_OPENAI:
            self.llm = openai.AsyncOpenAI(api_key=openai_api_key or "not-needed", base_url=openai_base_url)
            self.llm_type = "openai"

        # Build fallback chain (lazily initialized clients)
        self._fallback_chain = self._build_fallback_chain(llm_provider)
        # Cache for lazily-created fallback clients
        self._fallback_clients: Dict[str, Any] = {}

        fallback_names = [f["provider"] for f in self._fallback_chain]
        if fallback_names:
            print(f"[COGNITION] Fallback chain: {' -> '.join(fallback_names)}")

        print(f"[COGNITION] Initialized with {self.llm_type} backend")

    # === Fallback chain ===

    # Default models for each provider when used as fallback
    FALLBACK_MODELS = {
        "anthropic": "claude-sonnet-4-20250514",
        "openai": "gpt-4o-mini",
        "vertex": "gemini-2.0-flash-001",
    }

    def _build_fallback_chain(self, primary_provider: str) -> List[Dict]:
        """Build ordered list of fallback providers (excluding the primary)."""
        if not self.enable_fallback:
            return []

        # Priority order for fallbacks
        provider_order = ["anthropic", "openai", "vertex"]
        chain = []

        for provider in provider_order:
            if provider == primary_provider:
                continue
            if provider == "anthropic" and HAS_ANTHROPIC and self._anthropic_api_key:
                chain.append({
                    "provider": "anthropic",
                    "model": self.FALLBACK_MODELS["anthropic"],
                })
            elif provider == "openai" and HAS_OPENAI and self._openai_api_key:
                chain.append({
                    "provider": "openai",
                    "model": self.FALLBACK_MODELS["openai"],
                })
            elif provider == "vertex" and self.vertex_project:
                if HAS_VERTEX_GEMINI:
                    chain.append({
                        "provider": "vertex_gemini",
                        "model": self.FALLBACK_MODELS["vertex"],
                    })
                elif HAS_VERTEX_CLAUDE:
                    chain.append({
                        "provider": "vertex",
                        "model": "claude-3-5-sonnet-v2@20241022",
                    })

        return chain

    def _get_fallback_client(self, provider: str) -> Any:
        """Lazily create and cache a client for a fallback provider."""
        if provider in self._fallback_clients:
            return self._fallback_clients[provider]

        client = None
        if provider == "anthropic" and HAS_ANTHROPIC:
            client = AsyncAnthropic(api_key=self._anthropic_api_key)
        elif provider == "openai" and HAS_OPENAI:
            client = openai.AsyncOpenAI(
                api_key=self._openai_api_key or "not-needed",
                base_url=self._openai_base_url,
            )
        elif provider == "vertex" and HAS_VERTEX_CLAUDE:
            client = AnthropicVertex(
                project_id=self.vertex_project,
                region=self.vertex_location,
            )
        elif provider == "vertex_gemini" and HAS_VERTEX_GEMINI:
            vertexai.init(project=self.vertex_project, location=self.vertex_location)
            client = "gemini"

        if client is not None:
            self._fallback_clients[provider] = client
        return client

    # === Conversation memory ===

    def set_max_tokens(self, max_tokens: int) -> None:
        """Set max output tokens for LLM calls."""
        self._max_tokens = max(1, max_tokens)

    def set_temperature(self, temperature: float) -> None:
        """Set temperature for LLM calls."""
        self._temperature = max(0.0, min(2.0, temperature))

    def set_conversation_compressor(self, compressor) -> None:
        """Set conversation compressor for intelligent context management."""
        self._conversation_compressor = compressor

    def set_max_history(self, max_turns: int) -> None:
        """Set maximum conversation history turns to keep."""
        self._max_history_turns = max(0, max_turns)
        # Trim existing history if needed
        if len(self._conversation_history) > self._max_history_turns * 2:
            self._conversation_history = self._conversation_history[-(self._max_history_turns * 2):]

    def clear_conversation(self) -> int:
        """Clear conversation history. Returns number of messages cleared."""
        count = len(self._conversation_history)
        self._conversation_history = []
        return count

    def get_conversation_history(self) -> List[Dict[str, str]]:
        """Get the current conversation history."""
        return list(self._conversation_history)

    def _build_messages(self, user_prompt: str) -> List[Dict[str, str]]:
        """Build messages list including conversation history."""
        messages = []
        # Inject compressed context preamble if available
        if self._compressed_context_preamble:
            messages.append({"role": "user", "content": self._compressed_context_preamble})
            messages.append({"role": "assistant", "content": "Understood, I have the compressed context from earlier turns."})
        # Add conversation history (already trimmed)
        messages.extend(self._conversation_history)
        # Add current user message
        messages.append({"role": "user", "content": user_prompt})
        return messages

    async def _record_exchange(self, user_prompt: str, assistant_response: str) -> None:
        """Record a conversation exchange in history.

        Uses async LLM-powered compression when available for higher quality
        context preservation. Falls back to sync regex compression, then simple trim.
        """
        self._conversation_history.append({"role": "user", "content": user_prompt})
        self._conversation_history.append({"role": "assistant", "content": assistant_response})
        # Auto-compress if compressor is available, otherwise simple trim
        if self._conversation_compressor:
            # Try async LLM-powered compression first
            if hasattr(self._conversation_compressor, 'async_auto_compress_if_needed'):
                result = await self._conversation_compressor.async_auto_compress_if_needed(
                    self._conversation_history
                )
            else:
                result = self._conversation_compressor.auto_compress_if_needed(
                    self._conversation_history
                )
            if result.get("compressed"):
                self._conversation_history = result["messages"]
                self._compressed_context_preamble = result.get("context_preamble", "")
                if result.get("llm_powered"):
                    print(f"[COGNITION] LLM-powered compression saved ~{result.get('tokens_saved', 0)} tokens")
        else:
            # Fallback: simple trim to max history
            max_messages = self._max_history_turns * 2
            if len(self._conversation_history) > max_messages:
                self._conversation_history = self._conversation_history[-max_messages:]

    async def _call_provider(
        self, provider_type: str, model: str, client: Any,
        system_prompt: str, user_prompt: str
    ) -> tuple:
        """
        Call a specific LLM provider and return (response_text, token_usage).

        Includes conversation history for providers that support multi-turn
        messages (Anthropic, OpenAI, Vertex Claude). For other providers,
        history is injected into the prompt text.

        Args:
            provider_type: The provider type string
            model: Model identifier
            client: The provider client instance
            system_prompt: System prompt text
            user_prompt: User prompt text

        Returns:
            Tuple of (response_text, TokenUsage)

        Raises:
            Exception: If the provider call fails
        """
        messages = self._build_messages(user_prompt)

        if provider_type == "anthropic":
            response = await client.messages.create(
                model=model,
                max_tokens=self._max_tokens,
                system=system_prompt,
                messages=messages,
            )
            return response.content[0].text, TokenUsage(
                input_tokens=response.usage.input_tokens,
                output_tokens=response.usage.output_tokens,
            )

        elif provider_type == "vertex":
            response = client.messages.create(
                model=model,
                max_tokens=self._max_tokens,
                system=system_prompt,
                messages=messages,
            )
            return response.content[0].text, TokenUsage(
                input_tokens=response.usage.input_tokens,
                output_tokens=response.usage.output_tokens,
            )

        elif provider_type == "vertex_gemini":
            # Gemini doesn't support messages array natively - inject history into prompt
            history_text = self._format_history_as_text()
            full_prompt = f"{history_text}{user_prompt}" if history_text else user_prompt
            gen_model = GenerativeModel(model, system_instruction=system_prompt)
            response = await asyncio.to_thread(
                gen_model.generate_content,
                full_prompt,
                generation_config=GenerationConfig(
                    max_output_tokens=self._max_tokens,
                    temperature=self._temperature,
                )
            )
            usage = TokenUsage()
            if hasattr(response, 'usage_metadata'):
                usage = TokenUsage(
                    input_tokens=response.usage_metadata.prompt_token_count,
                    output_tokens=response.usage_metadata.candidates_token_count,
                )
            return response.text, usage

        elif provider_type == "openai":
            oai_messages = [{"role": "system", "content": system_prompt}]
            oai_messages.extend(messages)
            response = await client.chat.completions.create(
                model=model,
                max_tokens=self._max_tokens,
                messages=oai_messages,
            )
            usage = TokenUsage()
            if response.usage:
                usage = TokenUsage(
                    input_tokens=response.usage.prompt_tokens,
                    output_tokens=response.usage.completion_tokens,
                )
            return response.choices[0].message.content, usage

        elif provider_type == "vllm":
            history_text = self._format_history_as_text()
            full_prompt = f"{system_prompt}\n\n{history_text}User: {user_prompt}\n\nAssistant:"
            outputs = client.generate([full_prompt], self.sampling_params)
            text = outputs[0].outputs[0].text
            return text, TokenUsage(
                input_tokens=len(full_prompt.split()),
                output_tokens=len(text.split()),
            )

        elif provider_type == "transformers":
            tf_messages = [{"role": "system", "content": system_prompt}]
            tf_messages.extend(messages)
            inputs = self.tokenizer.apply_chat_template(
                tf_messages, add_generation_prompt=True, return_tensors="pt", return_dict=True
            ).to(client.device)

            with torch.no_grad():
                outputs = client.generate(
                    **inputs, max_new_tokens=self._max_tokens,
                    temperature=self._temperature, do_sample=True,
                )

            text = self.tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)
            return text, TokenUsage(
                input_tokens=inputs.input_ids.shape[1],
                output_tokens=len(outputs[0]) - inputs.input_ids.shape[1],
            )

        raise ValueError(f"Unsupported provider type: {provider_type}")

    def _format_history_as_text(self) -> str:
        """Format conversation history as text for providers that don't support messages."""
        if not self._conversation_history:
            return ""
        lines = []
        for msg in self._conversation_history:
            role = "User" if msg["role"] == "user" else "Assistant"
            # Truncate long messages in history
            content = msg["content"][:500]
            lines.append(f"{role}: {content}")
        return "\n".join(lines) + "\n\n"

    async def _call_with_fallback(self, system_prompt: str, user_prompt: str) -> tuple:
        """
        Call the primary LLM provider, falling back to alternatives on failure.

        Returns:
            Tuple of (response_text, token_usage, provider_type, model)
        """
        # Try primary provider first
        if self.llm is not None and self.llm_type != "none":
            try:
                text, usage = await self._call_provider(
                    self.llm_type, self.llm_model, self.llm,
                    system_prompt, user_prompt,
                )
                return text, usage, self.llm_type, self.llm_model
            except Exception as primary_error:
                self._fallback_stats["primary_failures"] += 1
                self._fallback_stats["last_fallback_error"] = str(primary_error)
                print(f"[COGNITION] Primary provider {self.llm_type} failed: {primary_error}")

                if not self.enable_fallback or not self._fallback_chain:
                    raise

                # Try fallback providers
                for fallback in self._fallback_chain:
                    fb_provider = fallback["provider"]
                    fb_model = fallback["model"]
                    self._fallback_stats["total_fallbacks"] += 1

                    try:
                        fb_client = self._get_fallback_client(fb_provider)
                        if fb_client is None:
                            continue

                        print(f"[COGNITION] Trying fallback: {fb_provider} ({fb_model})")
                        text, usage = await self._call_provider(
                            fb_provider, fb_model, fb_client,
                            system_prompt, user_prompt,
                        )
                        self._fallback_stats["fallback_successes"] += 1
                        self._fallback_stats["last_fallback_provider"] = fb_provider
                        print(f"[COGNITION] Fallback succeeded: {fb_provider}")
                        return text, usage, fb_provider, fb_model
                    except Exception as fb_error:
                        print(f"[COGNITION] Fallback {fb_provider} also failed: {fb_error}")
                        continue

                # All providers failed
                raise primary_error

        # No provider configured at all
        return "", TokenUsage(), "none", ""

    def get_fallback_stats(self) -> Dict:
        """Get statistics about fallback usage."""
        return dict(self._fallback_stats)

    # === Model access (for steering skill) ===

    def get_model(self) -> Any:
        """Get the underlying model (for steering skill)."""
        return self.llm

    def get_tokenizer(self) -> Any:
        """Get the tokenizer (for steering skill)."""
        return self.tokenizer

    def is_local_model(self) -> bool:
        """Check if running a local model (vLLM or Transformers)."""
        return self.llm_type in ("vllm", "transformers")

    def get_system_prompt(self) -> str:
        """Get the current system prompt."""
        base = self.system_prompt or DEFAULT_SYSTEM_PROMPT.format(
            name=self.agent_name,
            ticker=self.agent_ticker,
            specialty=self.agent_specialty,
        )
        if self._prompt_additions:
            return base + "\n" + "\n".join(self._prompt_additions)
        return base

    def set_system_prompt(self, new_prompt: str) -> None:
        """Replace the system prompt."""
        self.system_prompt = new_prompt
        self._prompt_additions = []

    def append_to_prompt(self, addition: str) -> None:
        """Append to the system prompt."""
        self._prompt_additions.append(addition)

    # === Model switching ===

    AVAILABLE_MODELS = {
        "anthropic": {
            "claude-sonnet-4-20250514": {"cost": "medium", "speed": "medium", "capability": "excellent"},
            "claude-3-5-sonnet-20241022": {"cost": "medium", "speed": "medium", "capability": "excellent"},
            "claude-3-5-haiku-20241022": {"cost": "low", "speed": "fast", "capability": "good"},
        },
        "openai": {
            "gpt-4o": {"cost": "medium", "speed": "medium", "capability": "excellent"},
            "gpt-4o-mini": {"cost": "low", "speed": "fast", "capability": "good"},
        },
        "vertex": {
            "gemini-2.0-flash-001": {"cost": "low", "speed": "fast", "capability": "good"},
            "gemini-1.5-pro-002": {"cost": "medium", "speed": "medium", "capability": "excellent"},
        },
    }

    def get_available_models(self) -> dict:
        """Get list of available models the agent can switch to."""
        available = {}
        if HAS_ANTHROPIC and self._anthropic_api_key:
            available["anthropic"] = self.AVAILABLE_MODELS["anthropic"]
        if HAS_OPENAI and self._openai_api_key:
            available["openai"] = self.AVAILABLE_MODELS["openai"]
        if self.vertex_project and (HAS_VERTEX_CLAUDE or HAS_VERTEX_GEMINI):
            available["vertex"] = self.AVAILABLE_MODELS["vertex"]
        return available

    def get_current_model(self) -> dict:
        """Get info about currently active model."""
        return {
            "model": self.llm_model,
            "provider": self.llm_type,
            "finetuned": self._finetuned_model_id is not None,
            "finetuned_model_id": self._finetuned_model_id,
        }

    def switch_model(self, new_model: str) -> bool:
        """Switch to a different model at runtime."""
        old_model = self.llm_model
        print(f"[COGNITION] Switching model: {old_model} -> {new_model}")

        try:
            if new_model.startswith("claude") and HAS_ANTHROPIC and self._anthropic_api_key:
                self.llm = AsyncAnthropic(api_key=self._anthropic_api_key)
                self.llm_type = "anthropic"
                self.llm_model = new_model
                return True

            elif new_model.startswith("gpt") or new_model.startswith("ft:"):
                if HAS_OPENAI:
                    self.llm = openai.AsyncOpenAI(
                        api_key=self._openai_api_key or "not-needed",
                        base_url=self._openai_base_url
                    )
                    self.llm_type = "openai"
                    self.llm_model = new_model
                    return True

            elif new_model.startswith("gemini") and self.vertex_project and HAS_VERTEX_GEMINI:
                vertexai.init(project=self.vertex_project, location=self.vertex_location)
                self.llm = "gemini"
                self.llm_type = "vertex_gemini"
                self.llm_model = new_model
                return True

            print(f"[COGNITION] Model {new_model} not available")
            return False
        except Exception as e:
            print(f"[COGNITION] Failed to switch model: {e}")
            return False

    # === Fine-tuning ===

    def record_training_example(self, prompt: str, response: str, outcome: str = "success") -> None:
        """Record a training example for fine-tuning."""
        if not hasattr(self, '_training_examples'):
            self._training_examples = []
        self._training_examples.append({
            "prompt": prompt,
            "response": response,
            "outcome": outcome,
            "timestamp": datetime.now().isoformat(),
        })
        print(f"[COGNITION] Recorded training example ({len(self._training_examples)} total)")

    def get_training_examples(self, outcome_filter: Optional[str] = None) -> list:
        """Get collected training examples."""
        if not hasattr(self, '_training_examples'):
            self._training_examples = []
        if outcome_filter:
            return [e for e in self._training_examples if e.get("outcome") == outcome_filter]
        return self._training_examples

    def clear_training_examples(self) -> int:
        """Clear all training examples."""
        if not hasattr(self, '_training_examples'):
            self._training_examples = []
        count = len(self._training_examples)
        self._training_examples = []
        return count

    def export_training_data(self, format: str = "jsonl") -> str:
        """Export training data in JSONL format for fine-tuning."""
        import json
        lines = []
        for ex in self.get_training_examples("success"):
            lines.append(json.dumps({
                "messages": [
                    {"role": "system", "content": self.get_system_prompt()},
                    {"role": "user", "content": ex["prompt"]},
                    {"role": "assistant", "content": ex["response"]},
                ]
            }))
        return "\n".join(lines)

    async def start_finetune(self, suffix: Optional[str] = None) -> dict:
        """Start a fine-tuning job with OpenAI."""
        examples = self.get_training_examples("success")
        if len(examples) < 10:
            return {"error": f"Need at least 10 examples, have {len(examples)}"}

        if not HAS_OPENAI or not self._openai_api_key:
            return {"error": "OpenAI API required for fine-tuning"}

        try:
            import tempfile
            client = openai.OpenAI(api_key=self._openai_api_key)

            # Write training data to temp file
            training_data = self.export_training_data()
            with tempfile.NamedTemporaryFile(mode='w', suffix='.jsonl', delete=False) as f:
                f.write(training_data)
                temp_path = f.name

            # Upload file
            with open(temp_path, 'rb') as f:
                file_response = client.files.create(file=f, purpose="fine-tune")

            # Start fine-tuning
            job = client.fine_tuning.jobs.create(
                training_file=file_response.id,
                model="gpt-4o-mini-2024-07-18",
                suffix=suffix or self.agent_ticker.lower(),
            )

            return {
                "job_id": job.id,
                "status": job.status,
                "model": job.model,
                "training_file": file_response.id,
            }
        except Exception as e:
            return {"error": str(e)}

    async def check_finetune_status(self, job_id: str) -> dict:
        """Check status of a fine-tuning job."""
        if not HAS_OPENAI or not self._openai_api_key:
            return {"error": "OpenAI API required"}

        try:
            client = openai.OpenAI(api_key=self._openai_api_key)
            job = client.fine_tuning.jobs.retrieve(job_id)

            result = {
                "job_id": job.id,
                "status": job.status,
                "model": job.model,
            }

            if job.fine_tuned_model:
                result["fine_tuned_model"] = job.fine_tuned_model
                self._finetuned_model_id = job.fine_tuned_model

            return result
        except Exception as e:
            return {"error": str(e)}

    def use_finetuned_model(self) -> bool:
        """Switch to the fine-tuned model."""
        if not hasattr(self, '_finetuned_model_id') or not self._finetuned_model_id:
            return False
        return self.switch_model(self._finetuned_model_id)

    async def think(self, state: AgentState) -> Decision:
        """
        Given current state, decide what action to take.

        Args:
            state: Current agent state including balance, tools, recent actions

        Returns:
            Decision with action to execute
        """
        # Build the prompt
        system_prompt = self.get_system_prompt()

        # Format tools
        tools_text = "\n".join([
            f"- {t['name']}: {t['description']}"
            for t in state.tools
        ])

        # Format recent actions with rich result feedback
        recent_text = ""
        if state.recent_actions:
            try:
                from .result_feedback import format_recent_actions
                recent_text = format_recent_actions(
                    state.recent_actions,
                    max_actions=5,
                    max_total_len=2000,
                    max_per_result=400,
                )
            except ImportError:
                # Fallback to simple format
                recent_text = "\nRecent actions:\n" + "\n".join([
                    f"- {a['tool']}: {a.get('result', {}).get('status', 'unknown')}"
                    for a in state.recent_actions[-5:]
                ])

        # Format pending events
        events_text = ""
        if state.pending_events:
            events_lines = ["\nPENDING EVENTS (react to these):"]
            for pe in state.pending_events[:5]:
                evt = pe.get("event", {})
                events_lines.append(
                    f"  - [{evt.get('topic', '?')}] from {evt.get('source', '?')}: "
                    f"{evt.get('data', {})} (reaction: {pe.get('reaction', 'handle it')})"
                )
            events_text = "\n".join(events_lines)

        user_prompt = f"""Current state:
- Balance: ${state.balance:.4f}
- Burn rate: ${state.burn_rate:.6f}/cycle
- Runway: {state.runway_hours:.1f} hours
- Cycle: {state.cycle}

Available tools:
{tools_text}
{recent_text}
{events_text}

{state.project_context}

{state.performance_context}

What action should you take? Respond with JSON: {{"tool": "skill:action", "params": {{}}, "reasoning": "why"}}"""

        # Call LLM with automatic fallback on failure
        try:
            response_text, token_usage, used_provider, used_model = await self._call_with_fallback(
                system_prompt, user_prompt
            )
        except Exception as e:
            print(f"[COGNITION] All LLM providers failed: {e}")
            return Decision(
                action=Action(tool="wait", params={}, reasoning=f"LLM error: {e}"),
                reasoning=f"All LLM providers failed: {e}",
                token_usage=TokenUsage(),
                api_cost_usd=0.0,
            )

        if not response_text:
            return Decision(
                action=Action(tool="wait", params={}),
                reasoning="No LLM backend available",
                token_usage=TokenUsage(),
                api_cost_usd=0.0,
            )

        # Record exchange in conversation memory
        await self._record_exchange(user_prompt, response_text)

        # Parse response
        action = self._parse_action(response_text)
        api_cost = calculate_api_cost(used_provider, used_model, token_usage)

        return Decision(
            action=action,
            reasoning=action.reasoning,
            token_usage=token_usage,
            api_cost_usd=api_cost
        )

    def _parse_action(self, response: str) -> Action:
        """Parse LLM response into an Action.

        Uses balanced brace matching to correctly handle nested JSON objects
        in params (e.g. params with nested dicts/lists).
        """
        # Strategy 1: Find JSON with balanced braces containing "tool"
        action = self._extract_json_action(response)
        if action:
            return action

        # Strategy 2: Try the whole response as JSON
        try:
            data = json.loads(response.strip())
            if isinstance(data, dict) and "tool" in data:
                return Action(
                    tool=data.get("tool", "wait"),
                    params=data.get("params", {}),
                    reasoning=data.get("reasoning", ""),
                )
        except (json.JSONDecodeError, ValueError):
            pass

        # Fallback - look for tool name
        tool_match = re.search(r'(\w+:\w+)', response)
        if tool_match:
            return Action(tool=tool_match.group(1), params={}, reasoning=response[:200])

        return Action(tool="wait", params={}, reasoning="Could not parse response")

    def _extract_json_action(self, text: str) -> Optional[Action]:
        """Extract a JSON action object using balanced brace matching.

        Handles nested objects like: {"tool": "x", "params": {"key": {"nested": true}}}
        """
        # Find all positions where '{' followed eventually by '"tool"'
        start = 0
        while start < len(text):
            brace_pos = text.find('{', start)
            if brace_pos == -1:
                break

            # Try to find balanced closing brace
            depth = 0
            in_string = False
            escape_next = False
            end_pos = None

            for i in range(brace_pos, len(text)):
                ch = text[i]
                if escape_next:
                    escape_next = False
                    continue
                if ch == '\\' and in_string:
                    escape_next = True
                    continue
                if ch == '"' and not escape_next:
                    in_string = not in_string
                    continue
                if in_string:
                    continue
                if ch == '{':
                    depth += 1
                elif ch == '}':
                    depth -= 1
                    if depth == 0:
                        end_pos = i
                        break

            if end_pos is None:
                start = brace_pos + 1
                continue

            candidate = text[brace_pos:end_pos + 1]
            try:
                data = json.loads(candidate)
                if isinstance(data, dict) and "tool" in data:
                    return Action(
                        tool=data.get("tool", "wait"),
                        params=data.get("params", {}),
                        reasoning=data.get("reasoning", ""),
                    )
            except (json.JSONDecodeError, ValueError):
                pass

            start = brace_pos + 1

        return None
